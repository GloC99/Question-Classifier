{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "from torch.autograd import Variable\n",
    "from global_parser import parser\n",
    "from glove import read_glove\n",
    "from vocabulary import Vocabulary\n",
    "from split_label import SplitLabel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/bow.config']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.read(\"../data/bow.config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWords(nn.Module):\n",
    "\n",
    "    def __init__(self, tagset_size, word_embeddings, file_path):\n",
    "        self.embedding_dim = int(parser['Network Structure']['word_embedding_dim'])\n",
    "        self.hidden_dim = int(self.embedding_dim*2/3)\n",
    "        # self.hidden_dim = 128\n",
    "\n",
    "        super(BagOfWords, self).__init__()\n",
    "\n",
    "        self.word_embeddings = word_embeddings\n",
    "\n",
    "        self.hidden2tag = nn.Linear(self.embedding_dim, self.hidden_dim)\n",
    "\n",
    "        self.activation_function1 = nn.Tanh()\n",
    "\n",
    "        self.hidden3tag = nn.Linear(self.hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        sentence_vector = self.word_embeddings(sentence)\n",
    "        sentence_vector = torch.mean(sentence_vector, dim=0)\n",
    "        tag_space = self.hidden2tag(sentence_vector)\n",
    "\n",
    "        tag_space = self.activation_function1(tag_space)\n",
    "\n",
    "        tag_space = self.hidden3tag(tag_space)\n",
    "\n",
    "        tag_scores = F.log_softmax(tag_space, dim = -1)\n",
    "\n",
    "        return tag_scores.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lrs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-56d1e74428bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0maccuracy_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mf1_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mpretrained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Options for model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pretrained'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mfreeze\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Options for model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'freeze'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lrs' is not defined"
     ]
    }
   ],
   "source": [
    "# train\n",
    "lrs = np.arange(0.01, 0.1, 0.01)\n",
    "accuracy_list = []\n",
    "f1_list = []\n",
    "for lr in lrs:\n",
    "    pretrained = eval(parser['Options for model']['pretrained'])\n",
    "    freeze = eval(parser['Options for model']['freeze'])\n",
    "    train_path = parser['Paths To Datasets And Evaluation']['path_train']\n",
    "    word_dim = parser['Network Structure']['word_embedding_dim']\n",
    "#     learning_rate = float(parser['Hyperparameters']['lr_param'])\n",
    "    model_type = parser['Options for model']['model']\n",
    "    epoch_number = int(parser['Model Settings']['epoch'])\n",
    "    if pretrained:\n",
    "        if freeze:\n",
    "            print(\"Training pretrained and freeze\", model_type, \"model...\")\n",
    "        else:\n",
    "            print(\"Training pretrained and fine-tuning\", model_type, \"model...\")\n",
    "        print()\n",
    "        glove_path = parser['Using pre-trained Embeddings']['path_pre_emb']\n",
    "        vec = read_glove(glove_path)\n",
    "        voca = Vocabulary(\"train\", word_dim)\n",
    "        voca.set_word_vector(vec)\n",
    "        voca.from_word2vect_word2ind()\n",
    "        voca.from_word2vect_wordEmbeddings(freeze)\n",
    "        word_emb = voca.get_word_embeddings()\n",
    "    else:\n",
    "        print(\"Training random\", model_type, \"model...\")\n",
    "        print()\n",
    "        voca = Vocabulary(\"train\", word_dim)\n",
    "        voca.setup('../data/train.txt')\n",
    "        word_emb = voca.get_word_embeddings()\n",
    "    features,labels = SplitLabel(train_path).generate_sentences()\n",
    "\n",
    "    tag_to_ix = {}\n",
    "    id = 0\n",
    "    for l in labels:\n",
    "        if l not in tag_to_ix.keys():\n",
    "            tag_to_ix[l] = id\n",
    "            id += 1\n",
    "\n",
    "    model = BagOfWords(len(tag_to_ix), word_emb, \"../data/bow.config\")\n",
    "\n",
    "    loss_function = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    #optimizer = optim.Adam(model.parameters(), lr=0.05)\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr= 0.01)\n",
    "\n",
    "    for epoch in range(epoch_number):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "        for sentence, tags in zip(features, labels):\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Tensors of word indices.\n",
    "            sentence_in = voca.get_sentence_ind(sentence,\"Bow\")\n",
    "            targets = torch.tensor([tag_to_ix[tags]],dtype=torch.long)\n",
    "\n",
    "            # Step 3. Run our forward pass.\n",
    "            tag_scores = model.forward(sentence_in)\n",
    "            #\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            #  calling optimizer.step()\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"Epoch\", epoch)\n",
    "        with torch.no_grad():\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            most_error_label = {}\n",
    "            for sentence, tags in zip(features,labels):\n",
    "                sentence_in = voca.get_sentence_ind(sentence,\"Bow\")\n",
    "                tag_scores = model.forward(sentence_in)\n",
    "\n",
    "                ind = torch.argmax(tag_scores)\n",
    "\n",
    "                y_pred.append(ind)\n",
    "                y_true.append(tag_to_ix[tags])\n",
    "\n",
    "                if tag_to_ix[tags] != ind:\n",
    "                    if tags not in most_error_label.keys():\n",
    "                        most_error_label[tags] = 1\n",
    "                    else:\n",
    "                        most_error_label[tags] += 1\n",
    "\n",
    "            most_error_label = sorted(most_error_label.items(), key=lambda item: item[1], reverse=True)[:3]\n",
    "\n",
    "            print(\"Accuracy\", accuracy_score(y_true,y_pred))\n",
    "            print(\"F1-score\", f1_score(y_true,y_pred,average='macro'))\n",
    "            # print(\"Confusion_matrix \\n\", confusion_matrix(y_true,y_pred))\n",
    "            # print(\"First three most frequent misclassifed label:\",most_error_label)\n",
    "            print()\n",
    "\n",
    "    torch.save(model, parser['Options for model']['model_save_path'])\n",
    "\n",
    "    tag2index_filepath = parser['Options for model']['tag2index_save_path']\n",
    "    tag2index_file = open(tag2index_filepath, \"wb\")\n",
    "    pickle.dump(tag_to_ix, tag2index_file)\n",
    "    tag2index_file.close()\n",
    "\n",
    "    voca_filepath = parser['Options for model']['voca_save_path']\n",
    "    voca_file = open(voca_filepath, \"wb\")\n",
    "    pickle.dump(voca, voca_file)\n",
    "    voca_file.close()\n",
    "    \n",
    "    \n",
    "    pretrained = eval(parser['Options for model']['pretrained'])\n",
    "    model_type = parser['Options for model']['model']\n",
    "    if pretrained:\n",
    "        freeze = eval(parser['Options for model']['freeze'])\n",
    "        if freeze:\n",
    "            print(\"Testing pretrained and freeze\", model_type, \"model...\")\n",
    "        else:\n",
    "            print(\"Testing pretrained and fine-tuning\", model_type, \"model...\")\n",
    "    else:\n",
    "        print(\"Testing random\", model_type, \"model...\")\n",
    "    print()\n",
    "\n",
    "    output_file = parser['Evaluation']['path_eval_result']\n",
    "    model_path = parser['Options for model']['model_save_path']\n",
    "    model = torch.load(model_path)\n",
    "    model.to('cpu')\n",
    "    tag2index_path = parser['Options for model']['tag2index_save_path']\n",
    "    tag2index_file = open(tag2index_path, \"rb\")\n",
    "    tag_to_ix = pickle.load(tag2index_file)\n",
    "\n",
    "    voca_filepath = parser['Options for model']['voca_save_path']\n",
    "    voca_file = open(voca_filepath, \"rb\")\n",
    "    voca = pickle.load(voca_file)\n",
    "\n",
    "    test_path = parser['Paths To Datasets And Evaluation']['path_test']\n",
    "    features, labels = SplitLabel(test_path).generate_sentences()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        most_error_label = {}\n",
    "        with open(output_file, 'w') as f:\n",
    "            for sentence, tags in zip(features,labels):\n",
    "                sentence_in = voca.get_sentence_ind(sentence,\"Bow\")\n",
    "                tag_scores = model.forward(sentence_in)\n",
    "                ind = torch.argmax(tag_scores)\n",
    "\n",
    "                predicted_label = list(tag_to_ix.keys())[list(tag_to_ix.values()).index(ind)]\n",
    "                f.write(str(predicted_label) + \"\\n\")\n",
    "                y_pred.append(ind)\n",
    "                y_true.append(tag_to_ix[tags])\n",
    "\n",
    "                if tag_to_ix[tags] != ind:\n",
    "                    if tags not in most_error_label.keys():\n",
    "                        most_error_label[tags] = 1\n",
    "                    else:\n",
    "                        most_error_label[tags] += 1\n",
    "\n",
    "            most_error_label = sorted(most_error_label.items(), key=lambda item: item[1], reverse=True)[:3]\n",
    "            accuracy = accuracy_score(y_true,y_pred)\n",
    "            f.write(\"Overall Accuracy of \" + str(model_type) + \" model:\" + str(accuracy))\n",
    "            f.close()\n",
    "\n",
    "        print(\"Accuracy\", accuracy)\n",
    "        print(\"F1-score\", f1_score(y_true,y_pred,average='macro'))\n",
    "        print(\"Confusion_matrix \\n\", confusion_matrix(y_true,y_pred))\n",
    "        print(\"First three most frequent misclassifed label:\",most_error_label)\n",
    "        accuracy_list.append(accuracy)\n",
    "        f1_list.append(f1_score(y_true,y_pred,average='macro'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracies among all learning rates:\", accuracy_list)\n",
    "print(\"accuracies among all learning rates:\", f1_list)\n",
    "lr_list = lrs.tolist()\n",
    "\n",
    "argmax_accu = accuracy_list.index(max(accuracy_list))\n",
    "argmax_f1 = f1_list.index(max(f1_list))\n",
    "ps_x_accu = lr_list[argmax_accu]\n",
    "ps_y_accu = accuracy_list[argmax_accu]\n",
    "\n",
    "ps_x_f1 = lr_list[argmax_f1]\n",
    "ps_y_f1 = f1_list[argmax_f1]\n",
    "\n",
    "print(\"the optimum one is \", ps_x_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.xlim([0, 0.6])\n",
    "plt.ylim([0, 1])\n",
    "plt.plot(lr_list,accuracy_list,label='Accuracy')\n",
    "plt.plot(lr_list,f1_list,label='F1 Scores')\n",
    "plt.plot([0, ps_x_accu],[ps_y_accu,ps_y_accu], c='r', linestyle = '--')\n",
    "plt.plot([ps_x_accu, ps_x_accu],[0,ps_y_accu], c='r', linestyle = '--')\n",
    "\n",
    "\n",
    "\n",
    "plt.plot([0, ps_x_f1],[ps_y_f1,ps_y_f1], c='y', linestyle = '--')\n",
    "plt.plot([ps_x_f1, ps_x_f1],[0,ps_y_f1], c='y', linestyle = '--')\n",
    "\n",
    "ps_coord_str_accu = \"max: \" + str((lr_list[argmax_accu],round(accuracy_list[argmax_accu],3)))\n",
    "plt.title(\"BOW Accuracy and F1 Score for pretrained model with fine-tunning\")\n",
    "plt.xlabel('Learning rate')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "# plt.show()\n",
    "plt.savefig(\"../visualization/out_bow_pre_fine.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "from global_parser import parser\n",
    "from glove import read_glove\n",
    "from split_label import SplitLabel\n",
    "from vocabulary import Vocabulary\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.read(\"../data/bilstm.config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, tagset_size, file_path, word_embeddings):\n",
    "        self.embedding_dim = int(parser['Network Structure']['word_embedding_dim'])\n",
    "        self.batch_size = int(parser['Network Structure']['batch_size'])\n",
    "\n",
    "        super(BiLSTMTagger, self).__init__()\n",
    "        self.hidden_dim = int(parser['Network Structure']['hidden_dim'])\n",
    "\n",
    "        self.bilstm = nn.LSTM(self.embedding_dim, self.hidden_dim, bidirectional=True)\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.hidden2tag = nn.Linear(self.hidden_dim * 2, tagset_size)\n",
    "\n",
    "    def forward(self, sentence_in):\n",
    "        embeds = self.word_embeddings(sentence_in)\n",
    "        bilstm_out, (h_n, c_n) = self.bilstm(embeds.view(len(embeds), 1, -1))\n",
    "\n",
    "        out = torch.hstack((h_n[-2, :, :], h_n[-1, :, :]))\n",
    "        tag_space = self.hidden2tag(out)\n",
    "\n",
    "        tag_scores = F.log_softmax(tag_space, dim = -1)\n",
    "\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pretrained = eval(parser['Options for model']['pretrained'])\n",
    "# freeze = eval(parser['Options for model']['freeze'])\n",
    "# train_path = parser['Paths To Datasets And Evaluation']['path_train']\n",
    "# word_dim = parser['Network Structure']['word_embedding_dim']\n",
    "# # learning_rate = float(parser['Hyperparameters']['lr_param'])\n",
    "# model_type = parser['Options for model']['model']\n",
    "# epoch_number = int(parser['Model Settings']['epoch'])\n",
    "# if pretrained:\n",
    "#     if freeze:\n",
    "#         print(\"Training pretrained and freeze\", model_type, \"model...\")\n",
    "#     else:\n",
    "#         print(\"Training pretrained and fine-tuning\", model_type, \"model...\")\n",
    "#     print()\n",
    "#     glove_path = parser['Using pre-trained Embeddings']['path_pre_emb']\n",
    "#     vec = read_glove(glove_path)\n",
    "#     voca = Vocabulary(\"train\", word_dim)\n",
    "#     voca.set_word_vector(vec)\n",
    "#     voca.from_word2vect_word2ind()\n",
    "#     voca.from_word2vect_wordEmbeddings(freeze)\n",
    "#     word_emb = voca.get_word_embeddings()\n",
    "\n",
    "# else:\n",
    "#     print(\"Training random\", model_type, \"model...\")\n",
    "#     print()\n",
    "#     voca = Vocabulary(\"train\", word_dim)\n",
    "#     voca.setup('../data/train.txt')\n",
    "#     word_emb = voca.get_word_embeddings()\n",
    "# features,labels = SplitLabel(train_path).generate_sentences()\n",
    "# tag_to_ix = {}\n",
    "# id = 0\n",
    "# for l in labels:\n",
    "#     if l not in tag_to_ix.keys():\n",
    "#         tag_to_ix[l] = id\n",
    "#         id += 1\n",
    "# model = BiLSTMTagger(len(tag_to_ix), '../data/bilstm.config', word_emb)\n",
    "\n",
    "# loss_function = nn.NLLLoss()\n",
    "\n",
    "\n",
    "\n",
    "accuracy_list = []\n",
    "f1_list = []\n",
    "# lrs = np.arange(0.01, 0.1, 0.01)\n",
    "lrs = np.array([ 0.01*1.58**j for j in range(0,11)])\n",
    "# lr_list = lrs.tolist()\n",
    "for lr in lrs:\n",
    "    pretrained = eval(parser['Options for model']['pretrained'])\n",
    "    freeze = eval(parser['Options for model']['freeze'])\n",
    "    train_path = parser['Paths To Datasets And Evaluation']['path_train']\n",
    "    word_dim = parser['Network Structure']['word_embedding_dim']\n",
    "    # learning_rate = float(parser['Hyperparameters']['lr_param'])\n",
    "    model_type = parser['Options for model']['model']\n",
    "    epoch_number = int(parser['Model Settings']['epoch'])\n",
    "    if pretrained:\n",
    "        if freeze:\n",
    "            print(\"Training pretrained and freeze\", model_type, \"model...\")\n",
    "        else:\n",
    "            print(\"Training pretrained and fine-tuning\", model_type, \"model...\")\n",
    "        print()\n",
    "        glove_path = parser['Using pre-trained Embeddings']['path_pre_emb']\n",
    "        vec = read_glove(glove_path)\n",
    "        voca = Vocabulary(\"train\", word_dim)\n",
    "        voca.set_word_vector(vec)\n",
    "        voca.from_word2vect_word2ind()\n",
    "        voca.from_word2vect_wordEmbeddings(freeze)\n",
    "        word_emb = voca.get_word_embeddings()\n",
    "\n",
    "    else:\n",
    "        print(\"Training random\", model_type, \"model...\")\n",
    "        print()\n",
    "        voca = Vocabulary(\"train\", word_dim)\n",
    "        voca.setup('../data/train.txt')\n",
    "        word_emb = voca.get_word_embeddings()\n",
    "    features,labels = SplitLabel(train_path).generate_sentences()\n",
    "    tag_to_ix = {}\n",
    "    id = 0\n",
    "    for l in labels:\n",
    "        if l not in tag_to_ix.keys():\n",
    "            tag_to_ix[l] = id\n",
    "            id += 1\n",
    "    model = BiLSTMTagger(len(tag_to_ix), '../data/bilstm.config', word_emb)\n",
    "\n",
    "    loss_function = nn.NLLLoss()\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    #optimizer = optim.Adam(model.parameters(), lr=0.05)\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr= 0.01)\n",
    "\n",
    "    for epoch in range(epoch_number):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "        for sentence, tags in zip(features, labels):\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Tensors of word indices.\n",
    "            sentence_in = voca.get_sentence_ind(sentence,\"Bilstm\")\n",
    "            targets = torch.tensor([tag_to_ix[tags]],dtype=torch.long)\n",
    "\n",
    "            # Step 3. Run our forward pass.\n",
    "            tag_scores = model.forward(sentence_in)\n",
    "            #\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            #  calling optimizer.step()\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"Epoch\", epoch)\n",
    "        with torch.no_grad():\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            most_error_label = {}\n",
    "            for sentence, tags in zip(features,labels):\n",
    "                sentence_in = voca.get_sentence_ind(sentence,\"Bilstm\")\n",
    "                tag_scores = model.forward(sentence_in)\n",
    "\n",
    "                ind = torch.argmax(tag_scores)\n",
    "\n",
    "                y_pred.append(ind)\n",
    "                y_true.append(tag_to_ix[tags])\n",
    "\n",
    "                if tag_to_ix[tags] != ind:\n",
    "                    if tags not in most_error_label.keys():\n",
    "                        most_error_label[tags] = 1\n",
    "                    else:\n",
    "                        most_error_label[tags] += 1\n",
    "\n",
    "\n",
    "    test_path = parser['Paths To Datasets And Evaluation']['path_dev']\n",
    "    features, labels = SplitLabel(test_path).generate_sentences()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        most_error_label = {}\n",
    "        for sentence, tags in zip(features,labels):\n",
    "            sentence_in = voca.get_sentence_ind(sentence,\"Bilstm\")\n",
    "            tag_scores = model.forward(sentence_in)\n",
    "            ind = torch.argmax(tag_scores)\n",
    "            y_pred.append(ind)\n",
    "            y_true.append(tag_to_ix[tags])\n",
    "\n",
    "            if tag_to_ix[tags] != ind:\n",
    "                if tags not in most_error_label.keys():\n",
    "                    most_error_label[tags] = 1\n",
    "                else:\n",
    "                    most_error_label[tags] += 1\n",
    "\n",
    "        most_error_label = sorted(most_error_label.items(), key=lambda item: item[1], reverse=True)[:3]\n",
    "        # print(\"Total number of labels:\", len(most_error_label)) ## 33\n",
    "        accuracy = accuracy_score(y_true,y_pred)\n",
    "        accuracy_list.append(accuracy)\n",
    "        f1 = f1_score(y_true,y_pred,average='macro')\n",
    "        f1_list.append(f1)\n",
    "        print(\"Accuracy\", accuracy)\n",
    "        print(\"F1-score\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"accuracies among all learning rates:\", accuracy_list)\n",
    "# print(\"accuracies among all learning rates:\", f1_list)\n",
    "# lr_list = lrs.tolist()\n",
    "\n",
    "# argmax_accu = accuracy_list.index(max(accuracy_list))\n",
    "# argmax_f1 = f1_list.index(max(f1_list))\n",
    "# ps_x_accu = lr_list[argmax_accu]\n",
    "# ps_y_accu = accuracy_list[argmax_accu]\n",
    "\n",
    "# ps_x_f1 = lr_list[argmax_f1]\n",
    "# ps_y_f1 = f1_list[argmax_f1]\n",
    "\n",
    "# print(\"the optimum one is \", ps_x_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracies among all learning rates:\", accuracy_list)\n",
    "print(\"accuracies among all learning rates:\", f1_list)\n",
    "lr_list = lrs.tolist()\n",
    "#index_pos = len(list_of_elems) - list_of_elems[::-1].index(elem) - 1\n",
    "argmax_accu = len(accuracy_list) - accuracy_list[::-1].index(max(accuracy_list)) -1\n",
    "argmax_f1 = len(f1_list) - f1_list[::-1].index(max(f1_list)) -1\n",
    "ps_x_accu = lr_list[argmax_accu]\n",
    "ps_y_accu = accuracy_list[argmax_accu]\n",
    "\n",
    "ps_x_f1 = lr_list[argmax_f1]\n",
    "ps_y_f1 = f1_list[argmax_f1]\n",
    "\n",
    "print(\"the optimum one is \", ps_x_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.plot(lr_list,accuracy_list,label='Accuracy')\n",
    "plt.plot(lr_list,f1_list,label='F1 Scores')\n",
    "plt.plot([0, ps_x_accu],[ps_y_accu,ps_y_accu], c='r', linestyle = '--')\n",
    "plt.plot([ps_x_accu, ps_x_accu],[0,ps_y_accu], c='r', linestyle = '--')\n",
    "\n",
    "\n",
    "\n",
    "plt.plot([0, ps_x_f1],[ps_y_f1,ps_y_f1], c='y', linestyle = '--')\n",
    "plt.plot([ps_x_f1, ps_x_f1],[0,ps_y_f1], c='y', linestyle = '--')\n",
    "\n",
    "ps_coord_str_accu = \"max: \" + str((lr_list[argmax_accu],round(accuracy_list[argmax_accu],3)))\n",
    "plt.title(\"BiLSTM Accuracy and F1 Score for pretrained model with fine-tuning\")\n",
    "plt.xlabel('Learning rate')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "# plt.show()\n",
    "plt.savefig(\"../visualization/out_bilstm_pre_fine1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,6))\n",
    "# plt.plot(*zip(*accuracy_store),label='Accuracy')\n",
    "# plt.plot(*zip(*f1_store),label='F1 Scores')\n",
    "# plt.title(\"BiLSTM Accuracy and F1 Score for \")\n",
    "# plt.xlabel('Learning rate')\n",
    "# plt.ylabel('Value')\n",
    "# plt.legend()\n",
    "# # plt.show()\n",
    "# plt.savefig(\"out_bilstm_rand.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
